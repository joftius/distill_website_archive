[
  {
    "path": "posts/2021-01-01-did-the-new-variant-of-covid-spread-through-schools/",
    "title": "Did the new variant of COVID spread through schools?",
    "description": "The common theory of the new COVID variant is that intrinsic biological factors make it more transmissible. An alternate theory attributes the same dynamics to social explanations like community spread among school-aged children.",
    "author": [
      {
        "name": "Joshua Loftus",
        "url": "https://joshualoftus.com/"
      }
    ],
    "date": "2021-01-01",
    "categories": [
      "statistics",
      "covid19",
      "biocentrism"
    ],
    "contents": "\nTL;DR\nLondon Boroughs with the highest recent infection rates also have younger populations\nSince September the infection rates have remained highest and increased fastest for school aged children\nThe observation that the new variant has not broken out in some regions could be explained by these regions experiencing their school-based outbreaks earlier in the season with a different variant\nCases and age across London Boroughs\nThe new SARS-nCov-2 variant drawing much attention from international media saw most of its recent growth in London and nearby regions, so let’s look at London first. We can plot the rate of new COVID cases after 1 November compared to the proportion of each Borough’s population under age 20 using PHE data on cases and ONS data for population demographic estimates (age, specifically)\n\n\n\n\n\n\n\nNote that this graph is an example of ecological correlation. Since the data has already been aggregated to the borough level relationships will appear more clear than they would be at finer scales of spatial resolution.\nThis plot is a rough summary of observational data, but it seems pretty clear that the rate of new COVID cases is lower in boroughs with proportionally less children, and higher in those with more children. This plot does not distinguish cases of the new variant from other cases. However, the focus here is on London where the new variant rapidly took over recently, so the majority of new cases here are of the new variant.\n\n\n\nAge group and infection rates\nThe ONS COVID survey provides estimates using random sampling of the population. Unlike data from cases, this should be less influenced by selection biases from people deciding whether or not to go get tested. Unfortunately there was no breakdown by age group and region, or by age group and new/old variant. But we can see from this data that the overall infection rates have remained consistently higher for school aged children and young adults. These younger groups also experienced the most rapid increase in infection rates in December during the same period that the new variant became more prevalent.\n\n\n\n\n\n\nThanks to Dan Cookson who brought my attention to this data1 from test results. I do not believe this data comes from a representative survey, so it could be biased by whatever factors cause people to get tested or not. On the plus side, it has more detailed spatial resolution for different age groups, so we can see test results by age group in different regions of England. Dan pointed out that some regions which are not currently seeing high infection rates among young people did so earlier in the season.\n\n\n\n\n\n\n\n\n\nComparing the two explanations\nWe saw an explosion of cases, centered in London, after the November lockdown ended. The biological theory attributes this to a new variant of the virus being more transmissible. The social theory I’m considering in this post attributes the same observation to community transmission enabled by schools remaining open, without any important biological difference.\nThere are other regions of England which are not experiencing the same explosion of cases as London. Both of these theories have to contend with this fact.\nThe biological theory would say that it’s only a matter of time before the new variant does take over in other regions\nThe school theory can point out that (at least some of) these other regions had outbreaks earlier in the season, so people may have changed school policy / behavior already\nPerhaps these earlier outbreaks among young people also explain the prevalence of a different strain that was predominant before the current variant broke out in the London area. The previous strain accounted for around 2/3 of cases between September and late October.\nAnd just for one closer comparison, here is London and another region which had an earlier outbreak:\n\n\n\nAge group and new variant odds\nSo far we’ve only considered data which does not distinguish between the different variants. Now we’ll conclude with another piece of evidence showing associations between age and the new variant specifically, although I do not have access to the data this is based on. This analysis by Imperial College, PHE, COG-UK and others, showed that among COVID cases those in age groups under age 20 were more likely to have tests consistent with the new variant. It also appears that adults in the age range most likely to have children in school might have slightly higher rates, and that all other age groups had relatively lower rates.\n\n\n\n\n(A) Age distribution of S- and S+ cases. (B) Ratio of S- to S+ proportions of cases in each 10 year band. Results shown are for weeks 46-51. Ages were capped at 80.\n\nConclusion\nSince the beginning of this story about a new variant I consistently stressed that even without knowledge of any different strains we already knew case numbers were increasing and more actions were necessary. I think this “new variant” story has been a distraction.\nThere is a tenable alternative theory to the current narrative about mutations conferring increased transmission. This B.1.1.7 variant might just be the strain that was most prevalent in schools in and around London going into the November lockdown. The fact that this variant continued to spread during the lockdown has been one of the key pieces of evidence for the claim that it is more contagious. But if schools remaining open explains that observation we can conclude two things. First, we may not need any additional biological explanation. And second, the biological explanation may in fact be a harmful distraction from the important conclusion that widespread community transmission can continue throughout lockdowns if schools remain open. Instead of focusing on random mutations we should be focusing on what concrete actions those with power can take to stop the spread.\nMore work needs to be done to further elaborate this social explanation and show it can better explain all the facts than the biological explanation.\nCode for reproducibility\nFor downloading the population data with counts of persons of various ages.\n\n\n# library(tidyverse)\ndownload_filename <- \"UK_population.xls\"\ndownload.file(\n  url = paste0(\n    \"https://www.ons.gov.uk/file?uri=\",\n    \"%2fpeoplepopulationandcommunity%2f\",\n    \"populationandmigration%2fpopulationestimates\",\n    \"%2fdatasets%2fpopulationestimatesforukenglandand\",\n    \"walesscotlandandnorthernireland%2fmid2019april2020\",\n    \"localauthoritydistrictcodes/ukmidyearestimates\",\n    \"20192020ladcodes.xls\"),\n  destfile = download_filename)\n\nUK_age_pop <- readxl::read_xls(download_filename,\n                                     col_names = TRUE,\n                                     sheet = \"MYE2 - Persons\",\n                                     range  = \"A5:CQ431\")\n\ndeleted <- file.remove(download_filename)\n\n\nn_age_0_19 <- UK_age_pop %>%\n  select(num_range(\"\", 0:19)) %>%\n  mutate(total = rowSums(.)) %>%\n  pull(total)\n\nUK_pop <- UK_age_pop %>%\n  select(matches(\"[A-Z]\")) %>%\n  mutate(pop_019 = n_age_0_19, pop = `All ages`) %>%\n  select(-`All ages`)\n\n\n\nFor downloading London COVID case data, filtering to recent cases, merging with population demographic data.\n\nlondon_covid <- read_csv(\n  paste0(\"https://data.london.gov.uk/download/coronavirus\",\n  \"--covid-19--cases/151e497c-a16e-414e-9e03-9e428f555ae9/\",\n  \"phe_cases_london_boroughs.csv\")\n\nlondon_recent <- london_covid %>%\n  filter(date > \"2020-11-01\") %>%\n  group_by(area_name) %>%\n  summarize(recent_cases = sum(new_cases))\n\nlondon_covid_age <- inner_join(\n  UK_pop, london_recent,\n  by = c(\"Name\" = \"area_name\")) %>%\n  mutate(\n    prop_child = pop_019 / pop,\n    recent_rate = recent_cases / pop)\n\nFor infection rates by age from ONS data\n\n\ndownload_filename <- \"covid_positivity_update.xlsx\"\ndownload.file(\n  url = paste0(\n  \"https://www.ons.gov.uk/file?uri=\",\n  \"%2fpeoplepopulationandcommunity%2f\",\n  \"healthandsocialcare%2fconditionsand\",\n  \"diseases%2fdatasets%2fcoronaviruscovid19\",\n  \"infectionsurveydata%2f2020/covid19infection\",\n  \"surveydatasets2020122423122020174305.xlsx\"),\n  destfile = download_filename)\n\nc19rates_update <- readxl::read_xlsx(download_filename,\n                              col_names = TRUE,\n                              sheet = \"1g\",\n                              range  = \"A7:V49\")\n\ndeleted <- file.remove(download_filename_update)\n\nnames(c19rates_update) <- c(\"date\", \ndo.call(paste, \nexpand.grid(\n  c(\"positive\", \"lower\", \"upper\"),\n  c(\"Age2to10\",\n    \"Age11to15\",\n    \"Age16toAge24\",\n    \"Age25to34\",\n    \"Age35to49\",\n    \"Age50to69\",\n    \"Age70+\")\n  )\n))\n\nc19_long_update <- c19rates_update %>%\n  pivot_longer(!date) %>%\n  separate(col = name, sep = \" \",\n           into = c(\"est\", \"age\")) %>%\n  pivot_wider(names_from = est, values_from = value)\n\n\n\nFor case data by age and region:\n\ncovid_specimens <- read_csv(\"download the giant file and put it here\")\ncovid_spec_age <- covid_specimens %>%\n  filter(areaType %in% c(\"region\", \"utla\"),\n         date > \"2020-09-01\",\n         age %in%  c(\n           \"0_4\",\n           \"5_9\",\n           \"10_14\",\n           \"15_19\",\n           \"20_24\",\n           \"25_29\",\n           \"30_34\",\n           \"35_39\",\n           \"40_44\",\n           \"45_49\",\n           \"50_54\",\n           \"55_59\",\n           \"60_64\",\n           \"65_69\",\n           \"70_74\",\n           \"75_79\",\n           \"80_84\",\n           \"85_89\"))\n\n# uncomment to free up memory\n#rm(covid_specimens)\n\ncovid_age <- covid_spec_age %>%\n  separate(col = age, sep = \"_\",\n           into = c(\"agel\", \"ageu\")) %>%\n  mutate(agel = as.numeric(agel),\n         ageu = as.numeric(ageu),\n         Age = cut_interval(agel, length = 10)]\n         ) %>%\n  group_by(areaType, areaName, date, Age) %>%\n  summarize(smoothed_cases = sum(newCasesBySpecimenDateRollingRate))\n\n#omg the name is too long\ncovid_age$areaName[covid_age$areaName ==\n            \"Yorkshire and The Humber\"] <- \"Yorkshire&Humber\"\n\ncovid_age %>%\n  filter(areaType == \"region\") %>%\n  select(areaName, date, Age, smoothed_cases) %>%\n  drop_na() %>%\n  ggplot(etc)\n\nFinally, for anyone curious for more of the details involved in the age odds ratios calculation this is the caption for Figure 4 in that paper:\n\nTo assess differences in the age distribution of VOC versus non-VOC cases, we considered S-and S+ case numbers in weeks 46-51 across NHS STP regions. Case numbers were standardised for differences in the population age composition in each area, weighted to compare S- cases from each NHS STP region and each epidemiological week with an equal number of S+ cases from that same STP and week (a case-control design), and aggregated over STP weeks. Accounting for binomial sampling variation and variation by area and week,we observe significantly more S- cases, our biomarker of VOC cases, among individuals aged 0-19…\n\n\n\n\n\n\n\n\n\n\n\nUnder supplementary data on the side↩︎\n",
    "preview": "posts/2021-01-01-did-the-new-variant-of-covid-spread-through-schools/did-the-new-variant-of-covid-spread-through-schools_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-01-02T12:56:22+00:00",
    "input_file": "did-the-new-variant-of-covid-spread-through-schools.utf8.md",
    "preview_width": 1344,
    "preview_height": 960
  },
  {
    "path": "posts/2020-12-28-is-the-new-variant-of-covid-really-more-transmissible/",
    "title": "Is the new variant of COVID really more transmissible?",
    "description": "A variant of COVID recently grew rapidly in London. Experts have warned this strain may be more transmissible, and governments have enacted more restrictions as a response. But the new variant has not spread rapidly in other locations.",
    "author": [
      {
        "name": "Joshua Loftus",
        "url": "https://joshualoftus.com/"
      }
    ],
    "date": "2020-12-28",
    "categories": [
      "statistics",
      "covid19",
      "biocentrism"
    ],
    "contents": "\nTL;DR\nA new strain of COVID-19 has spread rapidly in some regions, particularly London\nSome scientists believe the strain has a biological advantage that makes it more transmissible\nI believe there are non-biological explanations for these observations\nIn other regions the new strain has not spread rapidly, which is consistent with my theory\nWe have to slow the spread regardless and should focus on what does that most effectively\nThe new strain narrative\nIn the present context much has been made of the potential danger of the new variant of SARS-Cov-2. As reported in Science,\n\nOne reason to be concerned, Rambaut says, is that among the 17 mutations are eight in the gene that encodes the spike protein on the viral surface, two of which are particularly worrisome. One, called N501Y, has previously been shown to increase how tightly the protein binds to the angiotensin-converting enzyme 2 receptor, its entry point into human cells.\n\nWhen this story broke my initial reaction was skepticism. I thought this new strain could just be a random genetic marker that coincided with an outbreak of cases that was caused by behavior (or even “chance”) rather than by any important biological differences. We have more sophisticated measurement capability for biological data than for social data, and I worry that not enough has been done to rule out social explanations. For example, despite cases rising dramatically since September mobility indices have remained above the levels they saw in June, which were significantly higher than their April lows. So the UK’s November lockdown only temporarily slowed large scale community spread and did not get it under control.\n\nI think we could have had a fairly normal 2020 (as some countries have proven) with better public information campaigns, enforcement of masks and ventilation, and restrictions on the worst spreading events like mass gatherings for work/school, entertainment, or religion.\nWe already know that people are changing their behavior. Schools reopened. We did not need any special biological explanation for why cases have continued increasing. As we’ll see below, the most rapid increase has occurred in London where the estimated percent of the population testing positive has likely surpassed 2%. For comparison, most entire states in the US probably have higher rates. This is not to downplay the importance of taking measures to prevent the spread, but rather to question the necessity for any additional explanations (mutant strain) on top of what we already knew.\nThere is some evidence to suggest my initial theory is wrong and that these mutations are causally important. South Africa has seen a new strain with a similar mutation recently become the dominant strain while case numbers have increased even though it is currently summer there. I still think I may be right, and that data recently released in the UK reinforces my theory.\nThe last bit of background about the detection of this new strain also helps explain this data. From the Science article above:\n\nA fortunate coincidence helped show that B.1.1.7 (also called VUI-202012/01, for the first “variant under investigation” in December 2020), appears to be spreading faster than other variants in the United Kingdom. One of the polymerase chain reaction (PCR) tests used widely in the country, called TaqPath, normally detects pieces of three genes. But viruses with 69-70del lead to a negative signal for the gene encoding the spike gene; instead only two genes show up. That means PCR tests, which the United Kingdom conducts by the hundreds of thousands daily and which are far quicker and cheaper than sequencing the entire virus, can help keep track of B.1.1.7.\n\nIn other words, tests already being done in the UK can help identify whether a test result is consistent with being the new variant of COVID. I think this “fortunate coincidence” is actually the explanation for why the new variant became such a big story. You can see by watching this animation (press play) that different strains are always appearing and becoming more or less prevalent. This one happened to be easier to track, so certain decision makers (like NERVTAG, which includes no statisticians) saw this and conflated correlation with causation.\n\nI’m over-simplifying and over-generalizing because many of the experts (and even some of the journalists!) have stressed that we don’t yet know if the new variant is important.\nSo let’s look at these estimates of the percent of the UK population testing positive, broken down based on whether the test result is consistent with the new strain or otherwise.\nGetting the data (don’t read this)\nWe can download data with estimates of COVID-19 infection rates from the Office of National Statistics (which uses random sampling and adjusts for differential response with MRP, hence the smooth-looking curves in the graphs below). Unfortunately this data is stored in an excel spreadsheet (an abhorrent practice), so I’m showing my code below in case anyone else would like to access this data programmatically\n\nOne point about the ONS estimates that might be important: they’re adjusted to make the survey representative on age, sex and region, but not, for example, education. Their survey response rate was about 42%.\n\n\nlibrary(tidyverse)\ndownload_filename <- \"covid_positivity.xlsx\"\ndownload.file(\n  url = paste0(\n  \"https://www.ons.gov.uk/file?\",\n  \"uri=/peoplepopulationandcommunity/\",\n  \"healthandsocialcare/conditionsanddiseases\",\n  \"/adhocs/12708covid19infectionsurveyorf1abn\",\n  \"positivityrates/orf1abnmodellingadhoc.xlsx\"),\n  destfile = download_filename)\n\nc19rates <- readxl::read_xlsx(download_filename,\n                              col_names = FALSE,\n                              sheet = \"Data\",\n                              range  = \"A6:BI54\")\n\ndeleted <- file.remove(download_filename)\n\nnames(c19rates) <- c(\"date\", \ndo.call(paste, \nexpand.grid(\n  c(\"positive\", \"lower\", \"upper\"),\n  c(\"new\", \"other\"),\n  c(\"England\", \"NorthEast\", \"NorthWest\", \"YorkshHum\", \"EastMid\",\n    \"WestMid\", \"EastEngland\", \"London\", \"SouthEast\", \"SouthWest\"))\n))\n\nc19_long <- c19rates %>%\n  pivot_longer(!date) %>%\n  separate(col = name, sep = \" \",\n           into = c(\"est\", \"variant\", \"region\")) %>%\n  pivot_wider(names_from = est, values_from = value)\n\n\n\nPositivity rate plots for UK regions\nFirst let’s see the rates in two regions, the one where the new strain grew most rapidly and another region where it hasn’t.\n\n\n\n\n\n\n\nThe error bars are 95% credible intervals provided by the ONS, presumably these come from the MRP model used to adjust the survey.\nNext, here are all the regions sorted (top left to bottom right) in the order of the maximum estimated prevalence of the new strain.\n\n\n\nConclusion\nIf the new strain has a biological advantage that makes it more transmissible why isn’t it taking over in every region? This is not a rhetorical question. One real possibility is that only some of the test results that are consistent with the new strain actually are the new strain, and other ones are similar but don’t have the same set of important mutations. If this were true then regions where the new strain hasn’t taken over could just be places where it has not yet reached, or reached recently enough that it’s still early, i.e. these regions are a month or so behind London in the trajectory for the new strain.\nAnd let me repeat: none of this should be taken as skepticism about the importance of taking actions to control the spread of the pandemic. Whether this new strain is more transmissible or not, we already know there is continued widespread community transmission. We already know that millions more could suffer horribly, and die, and that our emergency and healthcare systems and economies will be devastated if this virus is allowed to spread out of control. We already knew that more action was necessary to slow the spread before we learned of this new variant. Still, the question of whether or not this new variant is more transmissible is important as a matter of scientific accuracy and public trust.\nTo be clear, I haven’t done the work of spelling out an alternate, behavioral explanation. Some have tried, but there just isn’t much good data available for that purpose. For example, mobility data apparently doesn’t show differences in activity levels between London and other parts of the UK throughout the relevant time period. Such data is incredibly coarse and not guaranteed to surface behavioral differences that could be important causal explanations.\nInstead of thinking about what data is available and convenient to access (like the mobility indexes), ask yourself what kind of social information you would like to know if it were included as a question in the ONS COVID infection survey. Does the household have any contacts with schools? Where do household members work? Has anyone eaten indoors at a restaurant in the last week? Questions like these linked with actual test results, rather than aggregated by region in a mobility index, could be of tremendous help in tracking down how the virus is spreading and figuring out how to slow it down. There’s almost no cost to simply asking a few more questions on a survey compared to the cost already going toward tests. It seems the only reasons this has not been done are a reluctance to know the answers to such questions (from e.g. desire to keep schools or restaurants open) and a reliance on big dumb data instead of intentional scientific thinking and deliberate study design.\nSupplemental plots\nTo be comprehensive I’ll also show the previous plots for the (separate) dataset that includes Scotland, Wales, and Northern Ireland.\n\n\ndownload_filename <- \"covid_positivity_SWNI.xlsx\"\ndownload.file(\n  url = paste0(\n    \"https://www.ons.gov.uk/file?uri=\",\n    \"/peoplepopulationandcommunity/\",\n    \"healthandsocialcare/conditionsanddiseases/\",\n    \"adhocs/12711coronaviruscovid19infectionsurvey\",\n    \"orf1abnmodellingscotlandwalesandnorthern\",\n    \"ireland/orf1abnmodellingadhocscotlandwales\",\n    \"andnorthernireland.xlsx\"),\n  destfile = download_filename)\n\nc19rates <- readxl::read_xlsx(download_filename,\n                              col_names = FALSE,\n                              sheet = \"Data\",\n                              range  = \"A6:S54\")\n\ndeleted <- file.remove(download_filename)\n\nnames(c19rates) <- c(\"date\", \ndo.call(paste, \nexpand.grid(\n  c(\"positive\", \"lower\", \"upper\"),\n  c(\"new\", \"other\"),\n  c(\"Scotland\", \"Wales\", \"NI\"))\n))\n\nc19_long <- c19rates %>%\n  pivot_longer(!date) %>%\n  separate(col = name, sep = \" \",\n           into = c(\"est\", \"variant\", \"region\")) %>%\n  pivot_wider(names_from = est, values_from = value)\n\nnew_regions <- c19_long %>%\n  filter(variant == \"new\") %>%\n  group_by(region) %>%\n  summarize(max = max(positive))\n\nc19_long_plot_SWNI <- c19_long %>% \n  left_join(new_regions, by = \"region\") %>%\n  mutate(region = fct_reorder(region, desc(max)))\n\n\n\n\n\n\nAnd as a reward for anyone who scrolled this far, the most extreme comparison: Wales and London.\n\n\n\nNew data update\nThe ONS has released new data up to 23 December so I’m updating this post. The new data shows more increases in the percent compatible with the new variant across England, which is now more consistent with the theory of increased transmissibility. However there is not yet any new data for Scotland, Wales, and Northern Ireland, so we can’t see if the trend is different now in Wales.\n\n\n\n\n\n\nThis new data also comes with revisions of the previous data, this is perhaps most obvious by looking at the West Midlands region.\n\n\n\nI have not yet seen the explanation for this but it could reflect the addition of back-dated test results and/or updates to parameters of the MRP model that adjusts survey responses.\nIn summary:\nNew estimates are more consistent with the new variant being more transmissible\nI still think it’s inconclusive. I’d like to see the explanation for the revision of previous numbers, and continue to watch other regions (including Wales, and other countries where the new variant has been detected)\nAnd as before, this information isn’t really necessary to know that more effective actions are required to stop the spread of (all strains of) the virus\n\n\n\n",
    "preview": "posts/2020-12-28-is-the-new-variant-of-covid-really-more-transmissible/is-the-new-variant-of-covid-really-more-transmissible_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2020-12-31T10:21:02+00:00",
    "input_file": "is-the-new-variant-of-covid-really-more-transmissible.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Relocating and rebuilding",
    "description": "Some retrospective on an eventful 2020, announcing a move from New York to London, a couple book recommendations, and a new design for this website.",
    "author": [
      {
        "name": "Joshua Loftus",
        "url": "https://joshualoftus.com/"
      }
    ],
    "date": "2020-12-21",
    "categories": [],
    "contents": "\nI’m rebuilding this site using distill. It might take some time but links to old posts and lecture notes will eventually work again. If something is missing feel free to contact me about it.\nI’m also relocating to start a position in the Statistics Department at the London School of Economics. I’ll be teaching an undergraduate course on machine learning for statistics majors, and will post some of the materials in the teaching section here.\nA lot of things happened in 2020 and changed my thinking in certain ways. Imagining counterfactual responses and trajectories of the coronavirus pandemic is illuminating. From the beginning, and even now after almost a year, many countries have failed almost completely at learning from the experiences of others. There are deep structural, institutional, and behavioral obstacles to coordinated intentional action, which become worse the longer capitalism has dominated a given society. Thinking of economies as a distributed artificial intelligence, capitalism is a kind of overfitting- maximizing the wealth and power of one small class of people at the expense of every other possible fitness metric. We’ve seen myriad negative consequences of that this year in a kind of speed-run preview of what will likely become much worse over the course of climate change.\nI read a few books this year that influenced my thinking a lot.\nThis Life by Martin Hägglund resonated with vague but deeply held feelings I’ve had as long as I can remember, and gives a coherent way to think explicitly about them. My own short version: the fact of inevitable death reminds us that time is precious, but if we look around it seems most of us are “wasting” a lot of our time or “spending” it in ways we would rather not, which is a great tragedy and maybe the most important moral problem facing humanity. Hägglund argues convincingly to define democratic socialism as nothing less than a revolution in values which is required for humanity to be meaningfully free.\nThe Secret of Our Success by Joseph Henrich was my first in-depth exposure to academic research about (cumulative) cultural evolution. The book is focused on the particular ways that humans are different from other animals. I’m obsessed with cumulative cultural evolution now, even if I did disagree with the book in some points (mainly where it seems to take evolutionary psychology seriously for some reason). I’m somewhat convinced now that “over-imitation” is our One Weird Trick for building civilization, a useful lens through which to approach learning (and, therefore, anything that can be learned), and (despite this paper) closely related to another useful concept called “causal opacity.”\nLeaving New York was not easy. I’m gonna miss it. There are so many places I need to visit again when I get chance. With vaccines rolling out now, hopefully it won’t be long until we’re all visiting each other much more often again.\nMap showing the area around Washington Square Park\n\n\n",
    "preview": "posts/welcome/nyumap.jpg",
    "last_modified": "2020-12-23T17:45:03+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-23-least-squares-as-springs/",
    "title": "Least squares as springs",
    "description": "Physics intuition for regression and other methods that minimize squared error. We can imagine springs pulling the model toward the data.",
    "author": [
      {
        "name": "Joshua Loftus",
        "url": "https://joshualoftus.com/"
      }
    ],
    "date": "2020-11-23",
    "categories": [
      "statistics",
      "machine learning",
      "physics"
    ],
    "contents": "\nDuring a recent “Zoom” lecture a student asked me a question about outliers. In the process of answering I realized something that I knew was true but had never seen explained in any sources. This post is my first attempt to develop an analogy that connects least squares methods, like regression or PCA, to physical intuition about springs or elastics.\nThe simplest version\nTo illustrate I will use data from gapminder conveniently provided in an R package by Jenny Bryan. Consider these two variables in the dataset, GDP per capita and life expectancy, plotted here in a standard scatterplot:\n\n\n\n(To have a less busy plot, one with fewer points, I’ve subsetted the data to the year 2007 and countries in Asia).\nNow we’re going to bring physical intuition into this by imagining these points as physical objects. For example, we can interpret the mean \\((\\bar x, \\bar y)\\) = 6252.7, 70.7 as the center of mass (if we assume every point has the same mass). This is the larger, blue point in the plot above. It’s not really important to think of mass specifically, just that this point is the center of the physical ensemble.\nWe need a few simple rules for our physical system:\nChanging the data is not allowed, i.e. the points are immovable.\nFor methods like regression (or PCA), we imagine a rigid object like a line (or hyperplane, in multiple regression) passing through the points, and the points exerting some force that changes the position of this body\nIf these methods use the standard least squares loss function then this force can be represented by springs or elastics, all of the same length and strength, attached at one end to the points and on the other end to the line (or hyperplane).\nThis line (or hyperplane) may bounce around at first as the springs pull it toward their anchoring points, but eventually it settles into an equilibrium where the forces from all the opposing springs are balanced out. In this equilibrium state we have two immediate consequences:\nThe rigid object (line or hyperplane) must pass through the center point. If it did not, there would be a net force acting on the object pulling it toward the center point, hence it would not yet be at equilibrium. In other words, all of the forces that would shift the object are exactly balanced out so it does not shift.\nThe same is true about torques, all of the forces that would rotate the object are balanced out.\nFor regression we need one more rule: the springs are guided so they only pull in directions aligned with the axis of the outcome variable, i.e. “vertically.” Let’s call this the vertical rule for reference later.\nHere is our simple example in picture form:\n\n\n\n(To create this plot I have copied the geom_spring example by Thomas Lin Pedersen from (the new version of!) the ggplot2 book).\nIf you are not immediately convinced that this intuition pays off, consider the issue that motivated me to think of this in the first place: influential outliers, points with high “leverage” in the statistical sense. The definition of statistical leverage is a bit complicated. But we can get the right intuition about it from a physical sense rather than those formal definitions. See the point in the bottom left of the plot above? Since it is pulling closer to the end of the line it has more leverage in the actual physical sense. This is the explanation I prefer to give students who aren’t taking a mathematical statistics course.\n(In case you’re wondering, that point is Afghanistan, a nation where the United States remains at war for almost two decades now…)\nLet’s see what happens when we move this one point so its leverage is being used to rotate the line clockwise instead of counter-clockwise. The old regression line, before this change, is shown as a faded line below for comparison.\n\n\n\nNotice the following changes: the center of mass has shifted a little, the point we moved is now a greater distance from the line so its force is larger, and the line has rotated noticeably due to the influence of this one point, even though our dataset has over 30 points, because that point has a lot of leverage.\nHooke’s law\nIs this intuition correct? Can we really think of least squares solutions (e.g. regression lines) as the equilibrium of a system of springs? Yes, or elastic rubber bands, or any material with linear elasticity, i.e. that follows Hooke’s law. Let’s consider how to apply this to regression, where for each point \\((x_i, y_i)\\) in the data, the spring attaches to the regression line at the point \\((x_i, \\hat y_i)\\). So the spring is stretched to a length of \\(|r_i| = |y_i - \\hat y_i|\\), and Hooke’s law says each spring is pulling on the line with a force proportional to this distance.\nWhen the system of the line and springs has stopped moving and settled into an equilibrium, this equilibrium position minimizes the energy of the system, which in this case is just the potential energy. The potential energy stored in a spring that is stretched a certain distance is the integral of the force over that distance, and since the force scales with the distance this means the energy scales with the squared distance. Hence, the equilibrium of this physical system minimizes the total potential energy:\n\\[\n\\sum_{i=1}^n \\frac{1}{2}k |r_i|^2 = \\frac{1}{2}k \\sum_{i=1}^n  (y_i - \\hat y_i)^2\n\\] where \\(k > 0\\) is the spring “stiffness” constant. The line that minimizes this is the same as the least squares regression line because the constants in front don’t change the minimizer. This argument works just as well for multiple regression as simple regression, even if we can’t visualize the higher dimensional plots involved. We can still think of springs that are pulling, perpendicular to the \\(y\\)-axis, on a hyperplane. In this case there can be torques in various different hyperplanes passing through the center of mass but they’re all balanced out, so the hyperplane doesn’t get “tilted” in any direction (if it’s in equilibrium).\nPrincipal components analysis\nAlthough PCA is often considered a more advanced topic than (simple) regression, its justification in our physical analogy is actually simpler. All we need to do is drop the vertical rule that was required for regression. In this case, the springs are allowed to rotate their angle of departure from the points, and their position of attachment to the line (or hyperplane) can slide to accommodate this change in angle. This results in an equilibrium where the springs are stretched as little possible. The total potential energy reaches a lower value because the springs are no longer constrained in which direction they can pull.\n\n\n\nI’ve plotted the line in a different color to emphasize that it’s not the regression line. Notice that the springs are no longer pulling vertically, instead they connect to the line at the point on the line which is nearest (as measured by overall distance, not just distance in the \\(y\\)-coordinate alone).\n(This is also called total least squares or a special case of Deming regression.)\nModel complexity/elasticity: machine learning or AI\nWe can keep building on this analogy by using it to understand more complex modeling methods with another very simple idea: elasticity of the model object itself. Instead of a rigid body like a line (or hyperplane), if we imagine it’s made of an elastic material that can be (locally) stretched and deformed then we can get more complex types of regression models.\nIn this analogy, simpler models like linear regression correspond to more rigid objects like an unbendable metal stick, and more complex models allow flexibility or elasticity when fitting the conditional expectation function like a bendable plastic stick, a rubber band, or even a tensionless string in the most complex case where the model is allowed to fit the points perfectly.\nFor example, here’s the local polynomial regression method (loess) used by default in the ggplot function stat_smooth:\n\n\n\nThis intuition is not only useful for learning the basic ideas of model complexity and machine learning, it can even be used for cutting-edge research. For example see this recent paper on elasticity in neural networks (by my friend Weijie Su and his coauthor!)\nConcluding thoughts\nI think there’s a lot of potential here for statistics education, especially for younger students. It wouldn’t be too difficult to create a physical classroom demonstration using springs or elastics, especially for the PCA case since that wouldn’t require any guiding tracks to constraint the force direction to be vertical.\nI also need to teach myself some more advanced plotting to gganimate these examples. If I succeed I’ll update this page.\nReferences\nI was surprised by how difficult it was to find any references for this elementary idea that provides physical intuition for such an important method as least squares regression. If you’re aware of any others please contact me because I would be glad to know of them.\n(Update) Thankfully, after this post was shared widely on Twitter people have helped alert me to more references which are now included in the list below.\nDwight, T. W. (1937). The fitting of linear regression lines by the method of least squares. The Forestry Chronicle, 13(4), 509-519.\nLevi, M. (2012). The mathematical mechanic: using physical reasoning to solve problems. Princeton University Press.\nExcerpt of the previous reference on stackexchange\nUpdate: Thanks to everyone who has contacted me with these additional references!\nTrey Goesh created this interactive simulation\nKieran Healy linked to this stackexchange answer with an awesome animation\nKameron Decker Harris pointed out this blog post showing splines as bendy physical sticks\nChris Schwarz at UIowa brought my attention to these lectures by the legendary Gil Strang\nMichael Friendly created a bibliography of some references to physics inspired stats models\nA lecture by Yann LeCun in the NYU deep learning course\nA recent ICLR paper (with video of a short talk) by Will Grathwohl and others\nA paper on deep learning by Jascha Sohl-Dickstein and others\nJohn Davis at UIndiana created this demo showing 3D splines (just imagine the vertical lines are springs!) and pointed to a different physics-inspired example by Gavin Simpson\n\n\n\n",
    "preview": "posts/2020-11-23-least-squares-as-springs/least-squares-as-springs_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2020-12-23T14:34:01+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-12-21-concise-defense-of-statistical-significance/",
    "title": "A concise defense of statistical significance",
    "description": "Recent arguments against the use of p-values and significance testing are mostly weak. The weak ones are actually arguments against making decisions or mistakes in general, which is impossible.",
    "author": [
      {
        "name": "Joshua Loftus",
        "url": "https://joshualoftus.com/"
      }
    ],
    "date": "2019-11-18",
    "categories": [
      "statistics",
      "reproducibility"
    ],
    "contents": "\nA letter, signed by over 800 scientists and published in Nature called for an end to using p-values to decide whether data refutes or supports a scientific hypothesis. The letter has received widespread coverage and reignited an old debate.\nWeaker arguments\nMost of the objections to p-values or the p < 0.05 threshold in these articles can be summarized into two categories:\nObjections that would apply to any method resulting in a yes/no decision\nObjections that would apply to any method with yes/no decisions that might be wrong\n\nA histogram of z-scores showing apparent publication bias from Erik van Zwet and Eric Cator’s “significance filter” paper\n\n\n\nBanning p-values or “p < 0.05” thresholds wouldn’t address these objections. We will still have to make decisions, we can’t just report a Bayes factor (or a p-value) and refuse to decide whether a drug trial should continue or not. So our decisions will still sometimes be wrong, and in both directions.\nStronger arguments\nThe last kind of objection is more sensible–though less often the focal point of this debate–and I would summarize it as:\nStatistical significance is not sufficient for practical significance, we need to know something about the effect size.\nI more or less agree with this, but it is not an objection to using p-values or p < 0.05, it’s an objection to misusing them or using them in isolation.\nAnd any method can be misused! A scientist could do many tests and only report the significant results, or they could compute many effect size estimates or Bayes factors and only report the largest ones.\nReal roots of the problem\nI think there are two underlying problems that the p-value debate is mostly a distraction from: the rote, uncritical application of any kind of statistical method (because, as Jack Schwartz put it, “… the simple-mindedness … to dress scientific brilliancies and scientific absurdities alike in the impressive uniform of formulae and theorems. Unfortunately however, an absurdity in uniform is far more persuasive than an absurdity unclad.”), and the incentive structure of scientific publication.\nFrom Why most published research findings are false (Ioannidis):\nCorollary 4: The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.\nCorollary 5: The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true.\nCorollary 6: The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.\nThe burden of the burden of proof\nLastly, it’s unfortunate but amusing that the Herald article mentioned many people misunderstand p-values, and the New Yorker article had this correction: “An earlier version of this article incorrectly defined p-value.”\nI sympathize with researchers who have done good work but can’t get it published because the result isn’t “significant.” This is a problem with publication standards, not statistical methods. It should be normal to publish negative or unimpressive results, otherwise the literature has little to teach us about what doesn’t work.\nIt’s also a resource issue. Collecting data is costly. There are many barriers to research other than arbitrary publication standards involving p-values, including many other arbitrary publication standards. While discontent has been focused on p-values, we may find ourselves suddenly facing de facto expectations of applying (often unnecessary) “artificial intelligence” to massive datasets in order to be published.\nNo particular statistical methodology is the cause or solution of these issues. (More and better statistics education is necessary!)\n\n\n\n",
    "preview": "posts/2020-12-21-concise-defense-of-statistical-significance/zscores.png",
    "last_modified": "2020-12-22T12:32:32+00:00",
    "input_file": {},
    "preview_width": 1040,
    "preview_height": 640
  },
  {
    "path": "posts/2020-12-22-a-conditional-approach-to-inference-after-model-selection/",
    "title": "A conditional approach to inference after model selection",
    "description": "Model selection can invalidate inference, such as significance tests, but statisticians have recently made progress developing methods to adjust for this bias. This post motivates a conditional approach with a simple screening rule example and introduces an R package that can compute adjusted significance tests.",
    "author": [
      {
        "name": "Joshua Loftus",
        "url": "https://joshualoftus.com/"
      }
    ],
    "date": "2018-02-26",
    "categories": [
      "statistics",
      "reproducibility",
      "selective inference",
      "R"
    ],
    "contents": "\nOverly honest research methods?\nA high profile case of a scientist retracting multiple papers due to p-hacking is recently gaining new attention due to a BuzzFeed article. Hopefully this will raise awareness and convince some that “keep hammering away at your data until you find want you were expecting” is a poor way to do science. But it’s possible to get things wrong, for the same reason, no matter how well-intentioned we may be. Even if we aren’t specifically seeking significant \\(p\\)-values, we can end up with biased results due to model selection. To see why, check out this earlier post on model selection bias. In this post I will describe a method to protect ourselves from this bias and compute adjusted \\(p\\)-values that are valid even though we’ve done model selection.\nModel selection with forward stepwise\nWe’ll pick up on the same example from a previous post:\n\n[…] consider the candy_rankings data from the fivethirtyeight package. The outcome variable is how often a given candy won in popularity matchups against other candies, and the predictor variables are various properties like whether or not the candy has chocolate, whether or not it’s fruit flavored, how sugary it is relative to other candies, and so on. There are 85 candies and 11 predictor variables in the dataset.\n\nThis time we’ll use the actual response variable, and run forward stepwise with AIC to pick a subset of the predictors that are highly predictive of the outcome (win percent). The resulting model, along with the biased \\(p\\)-values that popular software computes by default, is given below.\n\n                     Estimate Std. Error t value  Pr(>|t|)\nchocolateTRUE            19.1        3.6     5.3 0.0000009\nfruityTRUE                8.9        3.6     2.5 0.0147319\npeanutyalmondyTRUE        9.5        3.4     2.8 0.0073720\ncrispedricewaferTRUE      8.4        4.5     1.9 0.0652527\nhardTRUE                 -5.7        3.3    -1.7 0.0887116\nsugarpercent              8.0        4.1     1.9 0.0569311\n\nThese \\(p\\)-values are probably too small. The model selection method chose variables that seemed to be predictive of the outcome. The way \\(p\\)-values are computed is to consider how extreme the test statistic \\(T\\) is under the null hypothesis: \\(P(T > |t|)\\). But the model selection procedure picks variables that tend to have large observed values of \\(|t|\\), whether or not the null hypothesis for any one of them is true. How can we correct this? By the end of this post we’ll have adjusted \\(p\\)-values for this example, but first we need to understand how that adjustment works.\nAdjusted inference by conditionional probability\nOne approach, often referred to as selective inference, is to use conditional probabilities when computing \\(p\\)-values. Consider a random variable \\(M\\) representing which model is chosen by the model selection method, and let \\(m\\) be the observed value of \\(M\\) after running the algorithm (e.g. forward stepwise) on our data and getting a specific model. To compute conditionally adjusted \\(p\\)-values, we use \\[\nP(T > |t| \\mid M = m)\n\\] This conditional probability law usually has a simple form. For example, if the test statistic has a \\(t\\)-distribution, then the conditional law is usually a truncated \\(t\\). The specifics depend on the kind of model selection algorithm being used, and working them out is an area of ongoing research in statistics. During my PhD, I worked on a few cases (groups of variables, cross-validation) as part of my dissertation. To understand how/why the conditional law works, let’s consider an example that’s simpler than forward stepwise.\nMarginal screening example\nSuppose that instead of regression, we are solving a many-means problem where we want to select the largest effects. (Regression is similar to this when the design matrix is orthogonal). That is, we have many effects \\(z_i\\) for \\(i = 1, \\ldots, p\\) and the selection rule we use is to choose \\(z_i\\) if \\(z_i > 1\\). Then we want to do a one-sided test to see if those \\(z_i\\) are significantly large. We can think of this procedure as first screening away the majority of the data which we think is just noise, and then testing what made it through the screening procedure. I’ll generate data under the global null hypothesis where every effect is actually zero, and then plot some results.\n\n\n\nThis plot shows three things:\nA histogram of the selected effects.\nThe solid line shows the standard normal distribution. The upper tail areas of this distribution would be used for standard, unadjusted \\(p\\)-values.\nThe dashed line shows the truncated, conditional distribution \\(Z | Z > 1\\) for a standard normal \\(Z\\).\nIf we used the tail areas of the standard normal to compute \\(p\\)-values, these would be very small, even though the data was all generated under a global null hypothesis. This shows that the selection effect can invalidate inference, leading to a high type 1 error rate. But it’s pretty clear from the plot that the conditional distribution fits the data very well: if “significant” means extreme according to this distribution, then type 1 error rate would be small, it would match whatever nominal threshold \\(\\alpha\\) we decided.\nThe selectiveInference R package\nLet’s return to our first example about forward stepwise.\nThe details for computing \\(p\\)-values with conditional probability when model selection is more complicated–like using forward stepwise with AIC, or the LASSO with cross-validation, etc–are harder than the marginal screening case. But fortunately, there is an R package for it: the selectiveInference package available on CRAN! This package is still under development, and its authors include Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Stephen Reid and some other guy. The package currently does not support R formulas, so first we need to create a model.matrix, then we’ll run forward stepwise again with the fs function, then we’ll compute adjusted inference for the fitted model using fsInf. These last two functions are among several others in the selectiveInference package, including ones for doing all of this with the LASSO instead of forward stepwise.\n\n\nCall:\nfsInf(obj = fit, type = \"aic\", ntimes = 1)\n\nStandard deviation of noise (specified or estimated) sigma = 10.703\n\nTesting results at step = 6, with alpha = 0.100\n Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea\n   1 19.147   5.233   0.171   -14.225   37.970        0.05      0.050\n   4  9.483   2.697   0.559   -43.440   13.021        0.05      0.049\n   2  8.881   2.445   0.472   -35.763   26.424        0.05      0.050\n   6  8.385   1.833   0.515   -61.565   46.348        0.05      0.050\n  10  7.979   1.894   0.355   -44.222   72.414        0.05      0.050\n   7 -5.669  -1.690   0.358   -48.097   30.090        0.05      0.050\n\nEstimated stopping point from AIC rule = 6\n\nThere’s a lot of output here, but let’s focus on the adjusted \\(p\\)-values. We’ll put them together in a readout with the unadjusted ones:\n\n                     Estimate    Pr(>|t|) Adj.Pr(>|t|)\nchocolateTRUE           19.15 0.000000898        0.171\npeanutyalmondyTRUE       9.48 0.007372014        0.559\nfruityTRUE               8.88 0.014731896        0.472\ncrispedricewaferTRUE     8.39 0.065252715        0.515\nsugarpercent             7.98 0.056931100        0.355\nhardTRUE                -5.67 0.088711566        0.358\n\nThe adjusted \\(p\\)-values in the right column are all much larger than the unadjusted ones. In general, adjusted \\(p\\)-values will be larger, but by how much depends on a lot of specifics. In this case, and at the usual \\(\\alpha = 0.05\\) level, we went from 3 significant effects without adjusting for selection bias to zero. This reflects a fundamental tradeoff: the more we use the data to search for interesting things, the less surprised we must be about what we find. Otherwise, we may just be fooling ourselves, and maybe even end up needing to retract lots of papers…\nI hope this post was a useful introduction to the basic idea of using conditional probability to adjust for model selection, and makes more people aware of the selectiveInference package. This project is also on github. In future posts I will describe more examples, including other approaches to adjusting inference for model selection bias.\n\n\n\n",
    "preview": "posts/2020-12-22-a-conditional-approach-to-inference-after-model-selection/a-conditional-approach-to-inference-after-model-selection_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-12-22T12:33:35+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-12-22-model-selection-bias-invalidates-significance-tests/",
    "title": "Model selection bias invalidates significance tests",
    "description": "People often do regression model selection, either by hand or using algorithms like forward stepwise or the lasso. Sometimes they also report significance tests for the variables in the chosen model. But there's a problem: the reason for *p*-value significance may just be something called model selection bias.",
    "author": [
      {
        "name": "Joshua Loftus",
        "url": "https://joshualoftus.com/"
      }
    ],
    "date": "2018-02-13",
    "categories": [
      "statistics",
      "reproducibility"
    ],
    "contents": "\nSignificance tests and the reproducibility crisis\nSignificance testing may be one of the most popular statistical tools in science. Researchers and journals often treat significance–having a \\(p\\)-value \\(< 0.05\\)–as indication that a finding is true and perhaps publishable. But the tests used to compute many of the \\(p\\)-values people still rely on today were developed over a century ago, when “computer” was still a job title. Now that we have digital computers and it’s standard practice to collect and analyze “big data,” the mathematical assumptions underlying many classical significance tests are being pushed beyond their limits.\nThe journal Nature surveyed 1,576 scientists, asking them whether there is a reproducibility crisis, and found about 90% agreed. Across a range of disciplines, over 40% of respondents had tried and failed to reproduce at least one of their own experiments. What’s going on?\nAndrew Gelman suggested an analogy to describe how data analysis often proceeds in practice: the garden of forking paths. Researchers have to make many choices, and each choice represents a turn down a new fork in the path. Out of the many possible paths, how many end in significant, publishable findings, and how many in dead ends? Nobody wants to just give up when they reach a dead end, so maybe they retrace some of their steps and try a different path.\nModel selection might be the most labyrinthine part of this garden.\nWhat is model selection bias?\nTo illustrate with an example, we’ll consider the candy_rankings data from the fivethirtyeight package. The outcome variable is how often a given candy won in popularity matchups against other candies, and the predictor variables are various properties like whether or not the candy has chocolate, whether or not it’s fruit flavored, how sugary it is relative to other candies, and so on. There are 85 candies and 11 predictor variables in the dataset.\nYou can read the original article about this data here. In the article, they find that chocolate seems to be the most important variable for increasing the win percentage for a candy. We’ll approach the problem a little differently, and use model selection to pick a subset of the 11 predictor variables. Then we’ll report significance tests for the selected variables to show they really are important. This is a fairly common sequence of steps for analyzing data in linear regression.\n\nDetails for the curious: this particular method is a “greedy” algorithm that adds one predictor variable at a time into the model starting with the variable that has the highest correlation with the outcome, and stopping after choosing the number of variables in a data-driven way. These details are not important for the overall message of this post. Any model selection method can cause selection bias.\n\n\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     -0.19       0.19    -1.0    0.309\ncaramelTRUE     -0.54       0.26    -2.1    0.042\nsugarpercent     0.79       0.35     2.3    0.026\n\nAs you can see, the model selection algorithm chose a nice interpretable model with only a few variables, and the small \\(p\\)-values for significance tests tell us they are truly important and our model is good…\nExcept none of this is true! The reason I didn’t show the code for loading the dataset earlier is that I created the outcome variable with Y = rnorm(n) (this function in R generates \\(n\\) random numbers following the normal distribution). Instead of using the actual outcome variable from the data, I generated a random normal variable, totally independent of the predictors. And yet, we found a model with predictor variables that are significant.\nA small simulation study\nLet’s think about this to get some intuition. If we have one predictor variable \\(X_1\\) and we generate \\(Y\\) from rnorm(n), then the correlation (in absolute value) between \\(X_1\\) and \\(Y\\) will probably be small. Let’s do this many times and look at the distribution of (absolute) observed correlations:\n\n\n\nThese correlations are pretty small. The bulk of the distribution is less than 0.1. We probably wouldn’t mistake them for signal. But now, what if we have more predictor variables, and we select the best (absolute) correlation each time?\n\n\n\nWhen we have 11 predictor variables and we select the one with the largest correlation, the resulting distribution of correlations is larger. The bulk is now centered around 0.2. We might mistake these larger correlations for signal, when in fact they only reflect the fact that we searched through the garden of forking paths and chose the most impressive path out of 11. Can you imagine how much worse this problem would be if instead of 11 variables we had 100 to choose from, or 1000?\nReturning to the broader topic of model selection\nModel selection works by choosing variables that seem to be the best at predicting the outcome. The more variables we have to choose from, the more likely it is we find one that predicts \\(Y\\) very well, even just by pure randomness, as the plots above show. In general, this model selection bias gets larger as the size of our “search space” increases – meaning not only number of variables, but also the complexity of the fitted relationships (e.g. non-linear). And in general, significance tests for the coefficients in the selected model will be biased toward smaller \\(p\\)-values, potentially causing us to commit more Type 1 errors.\nThis is part of the reason why scientists are sometimes unable to reproduce their own experiments. When they got a significant result (\\(p < 0.05\\)) the first time, it may have been due to model selection bias. When they try to reproduce the result, the noise just goes in a different direction, and their original result disappears.\nSelection bias can make noise look like signal\nI hope you are convinced of this take home message. I began with a story about the candy dataset, instead of a simulation, because when people are analyzing their own data they never believe it might just be noise. The variables have names and meanings. When we see regression model output our first instinct may be to start interpreting the coefficients, becoming more confident about those that match our preconceptions and rationalizing about those that do not. But as we just saw, model selection makes it possible, and maybe even likely, to get nice looking significance test results even if the outcome variable is just pure noise.\nIn the candy example the outcome variable was noise, but that’s not essential. We could have had a real outcome variable but our predictor variables are all noise. Or we could have a mixture of real signals and noise. As long as there’s many variables, searching among them for the best makes the significant test results biased toward small \\(p\\)-values.\nIn future posts, I’ll describe three approaches to fixing this problem, including one that I’ve worked on in my own research.\n\n\n\n",
    "preview": "posts/2020-12-22-model-selection-bias-invalidates-significance-tests/model-selection-bias-invalidates-significance-tests_files/figure-html5/many_correlations-1.png",
    "last_modified": "2020-12-22T12:29:01+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
