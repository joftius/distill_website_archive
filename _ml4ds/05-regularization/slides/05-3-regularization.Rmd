---
title: "Machine learning"
subtitle: "Regularization"
author: "Joshua Loftus"
#institute: "LSE"
#date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "xaringan-themer.css", "../../../theme.css"]
#    seal: false    
    lib_dir: libs
    nature:
      titleSlideClass: ["top", "left"]
      countdown: 59000
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

class: inverse, middle, center

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
options(knitr.table.format = "html")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#2d708e",
  secondary_color = "#230433",
  link_color = "#55c667",
  text_bold_color = '#f68f46',
  title_slide_text_color = "#292929",
  title_slide_background_color = "#ffffff", #"#042333",
  title_slide_background_image = "../../../files/theme/LSE/spiral_stairs.jpg",
#    "https://upload.wikimedia.org/wikipedia/commons/1/1a/Workhouse_Nantwich.jpg",
  title_slide_background_size = "cover",
  ) #or contain
```

```{r xaringanextra, include=FALSE, warning=FALSE}
library(xaringanExtra)
xaringanExtra::use_animate_all("slide_left")
xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::style_panelset_tabs(font_family = "inherit")
```

```{r tidyverse, include=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
theme_set(theme_minimal(base_size = 22))
set.seed(1)
library(broom)
library(modelr)
```

<style type="text/css">
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
</style>

being biased is cool

---

## Low signal-to-noise ratio?


```{r echo = FALSE}
set.seed(1)
n <- 400
x <- 2*runif(n) 
#f <- function(x) -1 + 20*x - 100*x^2 + sin(1/x^4)
f <- function(x) -1 + 2*x - x^2 + sin(100*x)
y <- f(x) + (0.8) *rnorm(n)
noise <- data.frame(x=x, y=y)
noise_plot <- ggplot(noise, aes(x,y)) + 
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE, method = "lm", size = 2, linetype = "dashed") + 
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())
noise_plot
```

---

## A simple model vs the truth

.pull-left[
```{r highlight.output = c(5)}
models <- list(lm(y ~ x, data = noise),
               lm(y ~ f(x), data = noise)) #<<
map_dfr(models, glance)
```
]

--

.pull-right[

```{r echo = FALSE}
true_plot <- noise_plot + 
  ggtitle("The true model") +
  geom_line(aes(x,f(x)), color = "#00aa00", size = 1,
            data = data_grid(noise, x = seq_range(x, 1000)))
true_plot
```
]

`r rep(emo::ji("zany_face"), 19)`

---

## What function space makes sense?

The true model was complex

Based on the data alone we might think it's simple

We would underfit unless we somehow knew which function space to search...

---

### What if we search the same function space?

```{r}
set.seed(1); n <- 200; df <- data.frame(x = runif(n)) %>%
  mutate(y = -1 + 2*x + rnorm(n)) #<<
overfit <- function(k) glance(
  lm(y ~ x + sin(k*x), data = df))$r.squared
k_range <- 10:500
r_squareds <- map_dbl(k_range, overfit)
best_k <- k_range[which.max(r_squareds)]; best_k
```

$$
\mathbb E[Y | X = x] = \beta_0 + \beta_1 x + \beta_2 \sin(359x)
$$

Ok, why not?

---

## Are you skeptical of this model?

```{r echo = FALSE}
signal_plot <- ggplot(df, aes(x,y)) + 
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE, method = "lm", linetype = "dashed", color = "#00aa00", size = 2) + 
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())
signal_plot +
  ggtitle("The 'best' (overfit) model") +
  geom_line(aes(x, .fitted), color = "blue", size = 1,
            data = augment(best_model,
                           newdata = data_grid(df,
                                               x = seq_range(x, 1000))))
```

---

### What if I give you statistical inferences?

```{r highlight.output = c(6)}
g <- function(x) sin(best_k*x)
best_model <- lm(y ~ x + g(x), data = df)
tidy(best_model)
```

```{r highlight.output = c(5)}
models <- list(lm(y ~ x, data = df), best_model)
map_dfr(models, glance)
```



---

## So which is it?

When we aren't doing simulations we just have the data

.pull-left[
```{r echo = F}
noise_plot + ggtitle("Doomed to underfit")
```
]
.pull-right[
```{r echo = F}
signal_plot + ggtitle("Test data could prevent overfitting")
```
]

We don't know *a priori* the noise level, SNR, complexity...

---

### Validation / in-distribution generalization

If we had bigger data we could tell these two cases apart

.pull-left[
```{r echo = F}
set.seed(1)
n <- 10000
x <- 2*runif(n) 
#f <- function(x) -1 + 20*x - 100*x^2 + sin(1/x^4)
f <- function(x) -1 + 2*x - x^2 + sin(100*x)
y <- f(x) + (0.8) *rnorm(n)
noise <- data.frame(x=x, y=y)
noise_plot <- ggplot(noise, aes(x,y)) + 
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE, method = "lm", size = 2, linetype = "dashed") + 
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())
noise_plot
```
]
.pull-right[
```{r echo = F}
set.seed(1); n <- 5000; df <- data.frame(x = runif(n)) %>%
  mutate(y = -1 + 2*x + rnorm(n))
signal_plot <- ggplot(df, aes(x,y)) + 
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE, method = "lm", linetype = "dashed", color = "#00aa00", size = 2) + 
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())
signal_plot
```
]

---

## Variability overfitting

Deep learning "AI": large enough datasets to search vast spaces of complex functions without **variability problems** from overfitting

i.e. good in-distribution generalization (new data, same DGP)

--

But what about **bias problems** from overfitting?



### Perfectly smooth data

Credit: Reddit user [SurfingPolice](https://www.reddit.com/r/dataisbeautiful/comments/lkjlhy/relationship_between_film_length_and_average_imdb/gnk46yh/)

```{r echo = FALSE, cache = TRUE}
titles <- read_tsv("~/Downloads/archive/title.basics.tsv")
ratings <- read_tsv("~/Downloads/archive/title.ratings.tsv")
df <- titles %>%
  filter(titleType  == "movie") %>%
  inner_join(ratings, by = "tconst") %>%
  mutate(runtimeMinutes = parse_number(runtimeMinutes, na = c("\\N")))
rm(titles)
rm(ratings)
```

```{r echo = FALSE}
df %>%
  drop_na(runtimeMinutes) %>%
  filter(runtimeMinutes < 60 * 6, runtimeMinutes > 30, numVotes > 10) %>%
  mutate(roundedRuntimeMinutes = floor(runtimeMinutes),
         decade = startYear - startYear %% 10) %>%
  group_by(roundedRuntimeMinutes) %>%
  summarise(
    avgRatingPerTime = mean(averageRating),
    n_movies = n()
  ) %>%
  ggplot(aes(roundedRuntimeMinutes, avgRatingPerTime, color = n_movies)) + 
  geom_point() +
  guides(color = FALSE) +
  labs(x = "Runtime", y = "Average Rating") +
  scale_x_continuous(breaks = seq(0, 360, 30)) + 
  theme(axis.title = element_text(face = "bold")) 
```

---

## Disaggregated

```{r echo = FALSE}
df %>%
  drop_na(runtimeMinutes) %>%
  filter(runtimeMinutes < 60 * 6, runtimeMinutes > 30, numVotes > 10) %>%
  mutate(roundedRuntimeMinutes = floor(runtimeMinutes),
         decade = startYear - startYear %% 10) %>%
  filter(!is.na(decade), decade > 1930) %>%
  ggplot(aes(runtimeMinutes, averageRating)) + 
  geom_point(alpha = 0.1) +
  guides(color = FALSE) +
  labs(x = "Runtime", y = "Average Rating") 
```



